# Weka-toolkit-project
Weka toolkit project with python and mysql

                                                                        REGRESSION MODEL TOOLKIT


Keywords
Huge information Analytics, Machine Learning, Deep Learning, Python, Clustering, Statistician and Database.

Introduction:

Data Science is the fields of data visualization and data analytics representation in which crude information or the unstructured information is cleaned and prepared for the analysis reason. Information researchers utilize this information to get the necessary data for the future purpose. Information science utilizes numerous cycles and strategies on the huge information, the data might be organized or unstructured. We get information outlines accessible on the web is the crude data. It very well might be either in unstructured or semi organized arrangement. This information is additionally separated, cleaned and afterward number of required undertaking are performed for the examination with the utilization of the high programming language. This information is additionally dissected and afterward introduced for our better agreement and assessment.One should be evident that information science isn't tied in with making confounded models   or making marvelous perception neither one of the its is tied in with composing code however about          utilizing the information to make an effect for your organization, for this effect we need instruments like   convoluted information models and information representation. Regression is an example of a supervised machine learning technique in which you train a model using data that includes both the features and known values for the label, so that the model learns to fit the feature combinations to the label. Then, after training has been completed, you can use the trained model to predict labels for new items for which the label is unknown. You can use this toolkit to create regression models by using a drag and drop visual interface, without needing to write any code.

Tackling issues with information science:
When tackling a certifiable issue with Data Science, the initial move towards addressing it begins with Data Cleaning and Preprocessing. At the point when a Data Scientist is furnished with a dataset, it could be in an unstructured configuration with different irregularities. Sorting out the information and eliminating mistaken data makes it simpler to break down and draw bits of knowledge. This cycle includes the expulsion of repetitive information, the change of information in a recommended design, taking care of missing qualities and so on.

Phases of Data Science:
There are numerous devices used to deal with the enormous information accessible to us. Information researchers use programming apparatuses like Python, R, SAS, Java, and C/C++ to remove information from arranged information. Data scientists utilize numerous calculations and numerical models on the information.

Core Technologies:
These are the things I actually spent time thinking about and writing code for.
⦁	Backend/API
⦁	Frontend
⦁	Machine Learning
⦁	Virtualization
Backend: Django, a Python web system/bundle thingy that makes dealing with the data set and building an API a breeze. Add in here the related yet in fact separate Django REST structure for a standard API and Django Channels to help Web Socket associations and offbeat calculation (the models run in corresponding to the worker). 
Frontend: React a JavaScript web structure/library thingy from Facebook that makes pretty pictures in the program. 
Machine Learning: PYTorch a Machine Learning system/Python bundle gizmo for Neural Networks. Additionally from Facebook. 
Organization or Virtualization: Docker a virtual-machine thingy that characterizes itself by swearing it is anything but a virtual machine. For our motivations, it works similar to pip on steroids it mysteriously tackles all issues with introducing programming. 
Vital in light of the fact that you should later run your code on a worker, and introducing all the product again will be a torment. Your worker runs Linux and doesn't have a mouse.

Analysis Model:
Web applications abuse the route and collaboration offices of hyper printed HTML pages to give and ask data from the client. As an outcome, the investigation model proposed here stresses the route and connection designs over other structural viewpoints. Elective, more norm, design perspectives could be given for different viewpoints for example substance relationship graphs for information displaying. In the accompanying, a Web application is related to all the data that can be gotten from a given Web worker. Archives got to from various workers are viewed as outer to the given application. The examination of a Web application should be led in the advancement climate. Subsequently, some data that can't be gotten to by outside clients perusing the site is viewed as accessible. At times it tends to be separated consequently from antiquities utilized by the Web worker while in different cases it is thought to be physically given by the engineers. Pages can be static or dynamic. While the substance of a static Web page is fixed, the substance of a powerful page is figured at show time to the worker and may rely upon the data given by the client through info fields. The two subclasses of Web Page model such other options. At the point when the substance of a powerful page relies upon the estimation of a bunch of info factors, the trait utilization of class Dynamic Page contains them. In HTML client info can be assembled by misusing structures. A Web page can incorporate quite a few structures. Each structure is described by the info factors that are given by the client through it. Qualities gathered by structures are submitted to the Web worker by means of the unique connection present, whose target is consistently a powerful page.
This Regression Model Toolkit is used for different tasks like:
⦁	Data Loading
⦁	Statistics
⦁	visualization
⦁	Preprocessing
⦁	Data Splitting
⦁	Model Selection
⦁	Model Info
⦁	Results

Data Loading:
Data loading is the process of importing data files. The CSV file can be loading using other libraries as well, and we will look at a few approaches like:
⦁	Select Model
⦁	Read CSV or Excel file
CSV alludes to comma separated qualities which is a basic document design that helps in putting away even information structure. This CSV organization can be handily stacked into a Pandas data frame with the assistance of the read_csv work.

Statistics:
It is a numerical evaluation of chance that a particular events would occur. We utilize factual models to discover experiences given a specific arrangement of information. We can direct displaying on a moderately little arrangement of information just to attempt to comprehend the fundamental idea of the information. It is the process to measure the model accuracy. We use different measures for this purpose. For regression problem we will implement
⦁	Mean
⦁	Median
⦁	Mode
⦁	Mean square error
⦁	R mean square error
⦁	Mean absolute error

Data Preprocessing:
Data preprocessing is technique in which we transform our data regarding our problem. Transforming raw data so that it can compact with our model requirements.
⦁	Managing Missing Values in the Data 
⦁	Supplanting Missing Values 
⦁	Attributing Missing Values in information 
⦁	Working with Categorical Variables 
⦁	Working with Outliers 
⦁	Preprocessing Data for Model Building
⦁	Identify target features
⦁	Encoding
⦁	Model training
Genuine information is rarely ideal, it will have missing information cells, blunders, exceptions, errors in names, and substantially more. Information pre-handling is definitely not a solitary errand, yet numerous various undertakings, that should be performed bit by bit. The yield of one stage would be the contribution of the subsequent stage etc.

Data Splitting:
The train-test split is a method for assessing the presentation of an AI calculation. It tends to be utilized for grouping or relapse issues and can be utilized for any administered learning calculation.
⦁	Train data (a subset to prepare a model).
⦁	Test data (a subset to prepare a model).
You could envision cutting the single informational index as follows:

Data Visualization:
Visualization is the process of visualizing the numeric data in pictorial form to understand the data complexity. We apply this technique at multiple times in our system. We will use multiple visualization techniques:
⦁	Bar graph
⦁	Scatter plot
⦁	Pie chart
Model Selection:
In machine learning we choose the best model for our problem. You can select any model from this toolkit. For regression problem we will implement:
⦁	Linear regression
⦁	Multiple regression
⦁	Polynomial regression
⦁	Logistic regression

![first](https://user-images.githubusercontent.com/82101704/115679823-674fa380-a36c-11eb-9aba-58f69e81f536.png)
![Second](https://user-images.githubusercontent.com/82101704/115680693-3b80ed80-a36d-11eb-831f-38ae8b7b932f.png)
![Data-Preprocessing](https://user-images.githubusercontent.com/82101704/115680761-4c316380-a36d-11eb-83b4-5b981753f5d3.png)
![Missing-Vlues](https://user-images.githubusercontent.com/82101704/115680799-55bacb80-a36d-11eb-9065-d0b35ba3d43c.png)
![feature-selection](https://user-images.githubusercontent.com/82101704/115680958-7d119880-a36d-11eb-9c20-acc0fcbb035a.png)
![Data-Spliting](https://user-images.githubusercontent.com/82101704/115680997-87cc2d80-a36d-11eb-9c0f-b5727917ccb7.png)
![Model-Selection](https://user-images.githubusercontent.com/82101704/115681037-931f5900-a36d-11eb-915f-0ae257e9352c.png)

